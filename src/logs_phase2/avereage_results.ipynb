{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"./batch.json\"\n",
    "with open(input_file, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "# Load the JSON data from the file\n",
    "file_path = 'batch.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Define the list of candidate models and judge models\n",
    "candidate_model = [\"llama_vid\", \"gpt4v\", \"video_chatgpt\", \"mplug_owl_Video\", \"video_llava\"]\n",
    "judge_model = ['video_llava', 'llama_vid', 'gpt4v']  # Assuming 'mplug_owl_Video' is not a judge\n",
    "\n",
    "# Initialize the data structure for storing scores\n",
    "dataset2data = {}\n",
    "\n",
    "# Function to extract ratings from JSON-formatted strings\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_rating(rating_string):\n",
    "    \"\"\"\n",
    "    Extracts the rating number from a string using regex to handle different formats.\n",
    "    \n",
    "    Args:\n",
    "    rating_string (str): The input string containing the rating.\n",
    "    \n",
    "    Returns:\n",
    "    int: The extracted rating number or None if not found.\n",
    "    \"\"\"\n",
    "    # Regex pattern to find rating in different formats\n",
    "    # patterns = [\n",
    "    #     r\"rating['\\\"]?\\s*:\\s*['\\\"]?(\\d+)\",  # Matches JSON-like formats with 'rating' key\n",
    "    #     r\"^(\\d+)\\s*\\(\",                     # Matches formats like \"4 (Excellent)\"\n",
    "    #     r\"^(\\d+)$\",                         # Matches plain number formats like \"4\"\n",
    "    # ]\n",
    "    patterns = [\n",
    "        r\"rating['\\\"]?\\s*:\\s*['\\\"]?(\\d+)\",   # Matches JSON-like formats with 'rating' key\n",
    "        r\"rating['\\\"]?\\s*:\\s*['\\\"]?\\[\\[(\\d+)\\]\\]\",  # Matches JSON-like formats with rating enclosed in double brackets\n",
    "        r\"rating['\\\"]?\\s*:\\s*['\\\"]?\\[(\\d+)\\]\",  # Matches JSON-like formats with rating enclosed in single brackets\n",
    "        r\"^(\\d+)\\s*\\(\",                      # Matches formats like \"4 (Excellent)\"\n",
    "        r\"^(\\d+)$\",                          # Matches plain number formats like \"4\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, rating_string)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Iterate through each dataset and each sample, collecting ratings\n",
    "for dataset, samples in data.items():\n",
    "    results = {judge: {candidate: [] for candidate in candidate_model} for judge in judge_model}\n",
    "    for sample_id, sample_data in samples.items():\n",
    "        for judge in judge_model:\n",
    "            if judge in sample_data:\n",
    "                rating_info = sample_data[judge]\n",
    "                for candidate, rating_str in rating_info.items():\n",
    "                    rating = extract_rating(rating_str)\n",
    "                    if rating is not None:\n",
    "                        results[judge][candidate].append(rating)\n",
    "                    else:\n",
    "                        print(f'rating_str:{rating_str}')\n",
    "                        print(f'rating:{rating}')\n",
    "                        results[judge][candidate].append(None)\n",
    "    dataset2data[dataset] = results\n",
    "\n",
    "    # # Calculate average ratings for each candidate model under each judge for the dataset\n",
    "    # for judge, candidates in results.items():\n",
    "    #     for candidate, ratings in candidates.items():\n",
    "    #         if ratings:\n",
    "    #             average = round(mean(ratings), 2)\n",
    "    #         else:\n",
    "    #             average = None\n",
    "    #         if dataset not in dataset2data:\n",
    "    #             dataset2data[dataset] = {}\n",
    "    #         if judge not in dataset2data[dataset]:\n",
    "    #             dataset2data[dataset][judge] = {}\n",
    "    #             dataset2data[dataset][judge][candidate] = results[judge][candidate]\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"../logs_phase1_processed/batch_with_score.json\"\n",
    "with open(input_file, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, samples in data.items():\n",
    "    results = {'gpt3.5': {candidate: [] for candidate in candidate_model}}\n",
    "    for sample_id, sample_data in samples.items():\n",
    "        for candidate in candidate_model:\n",
    "            rating = sample_data[candidate][1]\n",
    "            results['gpt3.5'][candidate].append(rating)\n",
    "    dataset2data[dataset]['gpt3.5'] = results['gpt3.5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def get_agreement_with_baseline(dataset2data, dataset, judge, candidate):\n",
    "    ratings1 = dataset2data[dataset][judge][candidate]\n",
    "    ratings2 = dataset2data[dataset]['gpt3.5'][candidate]\n",
    "    # print(f\"ratings1:{ratings1}\")\n",
    "    # print(f\"ratings2:{ratings2}\")\n",
    "    # Calculate Weighted Kappa with linear weights\n",
    "    # kappa = cohen_kappa_score(ratings1, ratings2)\n",
    "    # print(\"Kappa:\", kappa)\n",
    "\n",
    "    # # Calculate Weighted Kappa with linear weights\n",
    "    # weighted_kappa_linear = cohen_kappa_score(ratings1, ratings2, weights='linear')\n",
    "    # print(\"Weighted Kappa (Linear):\", weighted_kappa_linear)\n",
    "\n",
    "    # Calculate Weighted Kappa with quadratic weights\n",
    "    weighted_kappa_quadratic = cohen_kappa_score(ratings1, ratings2, weights='quadratic')\n",
    "    # print(\"Weighted Kappa (Quadratic):\", weighted_kappa_quadratic)\n",
    "    return weighted_kappa_quadratic\n",
    "\n",
    "dataset2agreement = {}\n",
    "for dataset, data in dataset2data.items():\n",
    "    dataset2agreement[dataset] = {}\n",
    "    for judge in judge_model:\n",
    "        dataset2agreement[dataset][judge] = {}\n",
    "        judge_total = 0\n",
    "        for candidate in candidate_model:\n",
    "            dataset2agreement[dataset][judge][candidate] = get_agreement_with_baseline(dataset2data, dataset, judge, candidate)\n",
    "            judge_total += dataset2agreement[dataset][judge][candidate]\n",
    "        dataset2agreement[dataset][judge]['average'] = judge_total / len(dataset2agreement[dataset][judge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2average = {}\n",
    "judge_model.append('gpt3.5')\n",
    "for dataset, data in dataset2data.items():\n",
    "    dataset2average[dataset] = {}\n",
    "    for judge in judge_model:\n",
    "        dataset2average[dataset][judge] = {}\n",
    "        judge_total = 0\n",
    "        for candidate in candidate_model:\n",
    "            judge_total += sum(dataset2data[dataset][judge][candidate]) / len(dataset2data[dataset][judge][candidate])\n",
    "        dataset2average[dataset][judge] = judge_total / len(candidate_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvrr_continuity_and_object_instance_count': {'video_llava': 3.9570621468926555,\n",
       "  'llama_vid': 3.8237288135593226,\n",
       "  'gpt4v': 2.464406779661017,\n",
       "  'gpt3.5': 2.6203389830508472},\n",
       " 'cvrr_fine_grained_action_understanding': {'video_llava': 3.9678260869565216,\n",
       "  'llama_vid': 3.8521739130434782,\n",
       "  'gpt4v': 3.058260869565218,\n",
       "  'gpt3.5': 2.677391304347826},\n",
       " 'cvrr_interpretation_of_social_context': {'video_llava': 3.9685714285714284,\n",
       "  'llama_vid': 3.888571428571429,\n",
       "  'gpt4v': 2.6592857142857147,\n",
       "  'gpt3.5': 2.6014285714285714},\n",
       " 'cvrr_interpretation_of_visual_context': {'video_llava': 3.970695970695971,\n",
       "  'llama_vid': 3.892307692307692,\n",
       "  'gpt4v': 2.726739926739927,\n",
       "  'gpt3.5': 2.58974358974359},\n",
       " 'cvrr_multiple_actions_in_a_single_video': {'video_llava': 3.942767295597484,\n",
       "  'llama_vid': 3.752830188679245,\n",
       "  'gpt4v': 2.5628930817610067,\n",
       "  'gpt3.5': 2.3465408805031442},\n",
       " 'cvrr_non_existent_actions_with_existent_scene_depictions': {'video_llava': 3.9,\n",
       "  'llama_vid': 3.8594202898550725,\n",
       "  'gpt4v': 2.557971014492754,\n",
       "  'gpt3.5': 1.746376811594203},\n",
       " 'cvrr_non_existent_actions_with_non_existent_scene_depictions': {'video_llava': 3.9236111111111116,\n",
       "  'llama_vid': 3.7680555555555557,\n",
       "  'gpt4v': 2.3736111111111113,\n",
       "  'gpt3.5': 1.7708333333333333},\n",
       " 'cvrr_partial_actions': {'video_llava': 3.9640776699029123,\n",
       "  'llama_vid': 3.8155339805825244,\n",
       "  'gpt4v': 2.8223300970873786,\n",
       "  'gpt3.5': 2.287378640776699},\n",
       " 'cvrr_time_order_understanding': {'video_llava': 3.980263157894737,\n",
       "  'llama_vid': 3.844736842105263,\n",
       "  'gpt4v': 3.143421052631579,\n",
       "  'gpt3.5': 2.452631578947368},\n",
       " 'cvrr_understanding_emotional_context': {'video_llava': 3.971232876712329,\n",
       "  'llama_vid': 3.8404109589041093,\n",
       "  'gpt4v': 2.797260273972603,\n",
       "  'gpt3.5': 2.2095890410958905},\n",
       " 'cvrr_unusual_and_physically_anomalous_activities': {'video_llava': 3.9810526315789474,\n",
       "  'llama_vid': 3.838947368421052,\n",
       "  'gpt4v': 2.451578947368421,\n",
       "  'gpt3.5': 2.255789473684211}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset, data in dataset2agreement.items():\n",
    "#     for judge in judge_model:\n",
    "#         print(f'{dataset}-{judge}')\n",
    "#         print(dataset2agreement[dataset][judge]['average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def weighted_average_ratings(ratings, weight_dict):\n",
    "#     import numpy as np\n",
    "    \n",
    "#     # Extract weights for the judges in the order they appear in the ratings dictionary\n",
    "#     weights = [weight_dict[judge] for judge in ratings.keys()]\n",
    "    \n",
    "#     # Normalize weights to sum to 1\n",
    "#     total_weight = sum(weights)\n",
    "#     normalized_weights = [w / total_weight for w in weights]\n",
    "\n",
    "#     # Initialize list to store weighted ratings\n",
    "#     aggregated_ratings = []\n",
    "\n",
    "#     # Convert dictionary values to a list of lists for easier manipulation\n",
    "#     ratings_list = list(ratings.values())\n",
    "\n",
    "#     # Calculate the weighted average for each rating position\n",
    "#     for rating_tuple in zip(*ratings_list):\n",
    "#         weighted_avg = sum(r * w for r, w in zip(rating_tuple, normalized_weights))\n",
    "#         aggregated_ratings.append(round(weighted_avg, 2))\n",
    "\n",
    "#     return aggregated_ratings\n",
    "\n",
    "# Ratings from the judges\n",
    "# judge_ratings = {\n",
    "#     'video_llava': [4, 4, 4, 4, 4],\n",
    "#     'llama_vid': [4, 4, 4, 4, 4],\n",
    "#     'gpt4v': [2, 1, 1, 1, 2],\n",
    "# }\n",
    "\n",
    "# Dictionary of weights, assigning the highest weight to gpt4v\n",
    "# weights_dict = {\n",
    "#     'video_llava': 0.05,  # Lowest weight\n",
    "#     'llama_vid': 0.05,    # Moderate weight\n",
    "#     'gpt4v': 0.9         # Highest weight\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataset, data in dataset2data.items():\n",
    "#     dataset2data[dataset]['interpolated_judge'] = {}\n",
    "#     for judge in judge_model:\n",
    "#         for candidate in candidate_model:\n",
    "#             if candidate not in dataset2data[dataset]['interpolated_judge']:\n",
    "#                 dataset2data[dataset]['interpolated_judge'][candidate] = {}\n",
    "#             dataset2data[dataset]['interpolated_judge'][candidate][judge] = dataset2data[dataset][judge][candidate]\n",
    "\n",
    "# for dataset, data in dataset2data.items():\n",
    "#     for candidate in candidate_model:\n",
    "#         dataset2data[dataset]['interpolated_judge'][candidate] = weighted_average_ratings(dataset2data[dataset]['interpolated_judge'][candidate], weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
