{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_str:\n",
      "rating:None\n",
      "rating_str:\n",
      "rating:None\n",
      "rating_str:\n",
      "rating:None\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from statistics import mean\n",
    "\n",
    "# Load the JSON data from the file\n",
    "file_path = 'batch.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Define the list of candidate models and judge models\n",
    "candidate_model = [\"llama_vid\", \"gpt4v\", \"video_chatgpt\", \"mplug_owl_Video\", \"video_llava\"]\n",
    "judge_model = ['gpt4v']  # Assuming 'mplug_owl_Video' is not a judge\n",
    "\n",
    "# Initialize the data structure for storing scores\n",
    "dataset2data = {}\n",
    "\n",
    "# Function to extract ratings from JSON-formatted strings\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_rating(rating_string):\n",
    "    \"\"\"\n",
    "    Extracts the rating number from a string using regex to handle different formats.\n",
    "    \n",
    "    Args:\n",
    "    rating_string (str): The input string containing the rating.\n",
    "    \n",
    "    Returns:\n",
    "    int: The extracted rating number or None if not found.\n",
    "    \"\"\"\n",
    "    # Regex pattern to find rating in different formats\n",
    "    patterns = [\n",
    "        r\"rating['\\\"]?\\s*:\\s*['\\\"]?(\\d+)\",   # Matches JSON-like formats with 'rating' key\n",
    "        r\"rating['\\\"]?\\s*:\\s*['\\\"]?\\[\\[(\\d+)\\]\\]\",  # Matches JSON-like formats with rating enclosed in double brackets\n",
    "        r\"rating['\\\"]?\\s*:\\s*['\\\"]?\\[(\\d+)\\]\",  # Matches JSON-like formats with rating enclosed in single brackets\n",
    "        r\"^(\\d+)\\s*\\(\",                      # Matches formats like \"4 (Excellent)\"\n",
    "        r\"^(\\d+)$\",                          # Matches plain number formats like \"4\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, rating_string)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    \n",
    "    return None\n",
    "\n",
    "selected_dataset =( \n",
    "    \"cvrr_time_order_understanding\",\n",
    "    \"cvrr_continuity_and_object_instance_count\",\n",
    "    \"cvrr_fine_grained_action_understanding\",\n",
    "    \"cvrr_interpretation_of_social_context\",\n",
    "    \"cvrr_interpretation_of_visual_context\",\n",
    "    \"cvrr_multiple_actions_in_a_single_video\",\n",
    ")\n",
    "\n",
    "# Iterate through each dataset and each sample, collecting ratings\n",
    "for dataset, samples in data.items():\n",
    "    if dataset not in selected_dataset:\n",
    "        continue\n",
    "    results = {judge: {candidate: [] for candidate in candidate_model} for judge in judge_model}\n",
    "    for sample_id, sample_data in samples.items():\n",
    "        for judge in judge_model:\n",
    "            if judge in sample_data:\n",
    "                rating_info = sample_data[judge]\n",
    "                for candidate, rating_str in rating_info.items():\n",
    "                    rating = extract_rating(rating_str)\n",
    "                    if rating is not None:\n",
    "                        results[judge][candidate].append(rating)\n",
    "                    else:\n",
    "                        print(f'rating_str:{rating_str}')\n",
    "                        print(f'rating:{rating}')\n",
    "                        results[judge][candidate].append(3)\n",
    "    dataset2data[dataset] = results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset2data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# gpt_debate score\n",
    "input_file = \"../logs_phase1_processed/correct/batch_with_score.json\"\n",
    "with open(input_file, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset, samples in data.items():\n",
    "    if dataset not in selected_dataset:\n",
    "        continue\n",
    "    results = {'gpt_debate': {candidate: [] for candidate in candidate_model}}\n",
    "    for sample_id, sample_data in samples.items():\n",
    "        for candidate in candidate_model:\n",
    "            rating = sample_data[candidate][1]\n",
    "            results['gpt_debate'][candidate].append(rating)\n",
    "    dataset2data[dataset]['gpt_debate'] = results['gpt_debate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get statistics of judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt4v': {0: 0, 1: 2784, 2: 1275, 3: 708, 4: 1854, 5: 529},\n",
       " 'gpt_debate': {0: 0, 1: 3561, 2: 1427, 3: 551, 4: 1197, 5: 414}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_model_all = judge_model + ['gpt_debate']\n",
    "judge2statistics = {judge: {i:0 for i in range(6)} for judge in judge_model_all}\n",
    "for dataset, data in dataset2data.items():\n",
    "    for judge in judge_model_all:\n",
    "        for candidate in candidate_model:\n",
    "            for rating in dataset2data[dataset][judge][candidate]:\n",
    "                judge2statistics[judge][rating] += 1\n",
    "judge2statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvrr_continuity_and_object_instance_count': {'llama_vid': {'gpt4v': 2.07909604519774,\n",
       "   'gpt_debate': 1.9435028248587571},\n",
       "  'gpt4v': {'gpt4v': 2.6271186440677967, 'gpt_debate': 2.4745762711864407},\n",
       "  'video_chatgpt': {'gpt4v': 2.073446327683616,\n",
       "   'gpt_debate': 2.135593220338983},\n",
       "  'mplug_owl_Video': {'gpt4v': 2.1129943502824857,\n",
       "   'gpt_debate': 1.8531073446327684},\n",
       "  'video_llava': {'gpt4v': 2.1129943502824857,\n",
       "   'gpt_debate': 1.9661016949152543}},\n",
       " 'cvrr_fine_grained_action_understanding': {'llama_vid': {'gpt4v': 2.3086956521739133,\n",
       "   'gpt_debate': 1.934782608695652},\n",
       "  'gpt4v': {'gpt4v': 3.108695652173913, 'gpt_debate': 3.082608695652174},\n",
       "  'video_chatgpt': {'gpt4v': 2.1826086956521737,\n",
       "   'gpt_debate': 1.9652173913043478},\n",
       "  'mplug_owl_Video': {'gpt4v': 2.391304347826087,\n",
       "   'gpt_debate': 2.0695652173913044},\n",
       "  'video_llava': {'gpt4v': 2.3869565217391306,\n",
       "   'gpt_debate': 2.0652173913043477}},\n",
       " 'cvrr_interpretation_of_social_context': {'llama_vid': {'gpt4v': 1.8714285714285714,\n",
       "   'gpt_debate': 1.5357142857142858},\n",
       "  'gpt4v': {'gpt4v': 3.242857142857143, 'gpt_debate': 3.092857142857143},\n",
       "  'video_chatgpt': {'gpt4v': 2.2035714285714287,\n",
       "   'gpt_debate': 2.1714285714285713},\n",
       "  'mplug_owl_Video': {'gpt4v': 2.2607142857142857,\n",
       "   'gpt_debate': 1.9964285714285714},\n",
       "  'video_llava': {'gpt4v': 2.0357142857142856,\n",
       "   'gpt_debate': 1.8928571428571428}},\n",
       " 'cvrr_interpretation_of_visual_context': {'llama_vid': {'gpt4v': 2.4761904761904763,\n",
       "   'gpt_debate': 1.7545787545787546},\n",
       "  'gpt4v': {'gpt4v': 3.5164835164835164, 'gpt_debate': 2.9816849816849818},\n",
       "  'video_chatgpt': {'gpt4v': 2.63003663003663,\n",
       "   'gpt_debate': 2.172161172161172},\n",
       "  'mplug_owl_Video': {'gpt4v': 2.6153846153846154,\n",
       "   'gpt_debate': 2.0732600732600734},\n",
       "  'video_llava': {'gpt4v': 2.402930402930403,\n",
       "   'gpt_debate': 1.9377289377289377}},\n",
       " 'cvrr_multiple_actions_in_a_single_video': {'llama_vid': {'gpt4v': 2.1226415094339623,\n",
       "   'gpt_debate': 1.7358490566037736},\n",
       "  'gpt4v': {'gpt4v': 2.9842767295597485, 'gpt_debate': 2.3364779874213837},\n",
       "  'video_chatgpt': {'gpt4v': 2.2641509433962264,\n",
       "   'gpt_debate': 2.0660377358490565},\n",
       "  'mplug_owl_Video': {'gpt4v': 2.091194968553459,\n",
       "   'gpt_debate': 1.6540880503144655},\n",
       "  'video_llava': {'gpt4v': 2.1132075471698113,\n",
       "   'gpt_debate': 1.7044025157232705}},\n",
       " 'cvrr_time_order_understanding': {'llama_vid': {'gpt4v': 2.6447368421052633,\n",
       "   'gpt_debate': 1.894736842105263},\n",
       "  'gpt4v': {'gpt4v': 3.1907894736842106, 'gpt_debate': 2.401315789473684},\n",
       "  'video_chatgpt': {'gpt4v': 2.5197368421052633,\n",
       "   'gpt_debate': 1.9342105263157894},\n",
       "  'mplug_owl_Video': {'gpt4v': 2.7039473684210527,\n",
       "   'gpt_debate': 1.9407894736842106},\n",
       "  'video_llava': {'gpt4v': 2.539473684210526,\n",
       "   'gpt_debate': 1.9539473684210527}}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judge_model_all = judge_model + ['gpt_debate']\n",
    "candidate2statistics = {dataset: {candidate: {judge:[] for judge in judge_model_all} for candidate in candidate_model} for dataset in dataset2data}\n",
    "\n",
    "for dataset, data in dataset2data.items():\n",
    "    if dataset not in selected_dataset:\n",
    "        continue\n",
    "    for judge in judge_model_all:\n",
    "        for candidate in candidate_model:\n",
    "            for rating in dataset2data[dataset][judge][candidate]:\n",
    "                candidate2statistics[dataset][candidate][judge].append(rating)\n",
    "            candidate2statistics[dataset][candidate][judge] = sum(candidate2statistics[dataset][candidate][judge]) / len(candidate2statistics[dataset][candidate][judge])\n",
    "candidate2statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "def get_agreement_with_baseline(dataset2data, dataset, judge, candidate):\n",
    "    ratings1 = dataset2data[dataset][judge][candidate]\n",
    "    ratings2 = dataset2data[dataset]['gpt_debate'][candidate]\n",
    "    # print(f\"ratings1:{ratings1}\")\n",
    "    # print(f\"ratings2:{ratings2}\")\n",
    "    # Calculate Weighted Kappa with linear weights\n",
    "    # kappa = cohen_kappa_score(ratings1, ratings2)\n",
    "    # print(\"Kappa:\", kappa)\n",
    "\n",
    "    # # Calculate Weighted Kappa with linear weights\n",
    "    # weighted_kappa_linear = cohen_kappa_score(ratings1, ratings2, weights='linear')\n",
    "    # print(\"Weighted Kappa (Linear):\", weighted_kappa_linear)\n",
    "\n",
    "    # Calculate Weighted Kappa with quadratic weights\n",
    "    weighted_kappa_quadratic = cohen_kappa_score(ratings1, ratings2, weights='quadratic')\n",
    "    # print(\"Weighted Kappa (Quadratic):\", weighted_kappa_quadratic)\n",
    "    return weighted_kappa_quadratic\n",
    "\n",
    "dataset2agreement = {}\n",
    "for dataset, data in dataset2data.items():\n",
    "    dataset2agreement[dataset] = {}\n",
    "    for judge in judge_model:\n",
    "        dataset2agreement[dataset][judge] = {}\n",
    "        judge_total = 0\n",
    "        for candidate in candidate_model:\n",
    "            dataset2agreement[dataset][judge][candidate] = get_agreement_with_baseline(dataset2data, dataset, judge, candidate)\n",
    "            judge_total += dataset2agreement[dataset][judge][candidate]\n",
    "        dataset2agreement[dataset][judge]['average'] = judge_total / len(dataset2agreement[dataset][judge])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvrr_continuity_and_object_instance_count': {'gpt4v': {'llama_vid': 0.29907670153741905,\n",
       "   'gpt4v': 0.41423291542865426,\n",
       "   'video_chatgpt': 0.3889085224955824,\n",
       "   'mplug_owl_Video': 0.2515637483193358,\n",
       "   'video_llava': 0.30288946130585725,\n",
       "   'average': 0.33133426981736974}},\n",
       " 'cvrr_fine_grained_action_understanding': {'gpt4v': {'llama_vid': 0.48335431881138247,\n",
       "   'gpt4v': 0.3307940538538857,\n",
       "   'video_chatgpt': 0.6069815696326133,\n",
       "   'mplug_owl_Video': 0.5221222521101938,\n",
       "   'video_llava': 0.5766174291743367,\n",
       "   'average': 0.5039739247164825}},\n",
       " 'cvrr_interpretation_of_social_context': {'gpt4v': {'llama_vid': 0.5458612975391499,\n",
       "   'gpt4v': 0.45141065830721006,\n",
       "   'video_chatgpt': 0.6193746131855825,\n",
       "   'mplug_owl_Video': 0.5943039550836522,\n",
       "   'video_llava': 0.6005217674873635,\n",
       "   'average': 0.5622944583205917}},\n",
       " 'cvrr_interpretation_of_visual_context': {'gpt4v': {'llama_vid': 0.2916566134246972,\n",
       "   'gpt4v': 0.400340758186966,\n",
       "   'video_chatgpt': 0.41180602939742994,\n",
       "   'mplug_owl_Video': 0.5015974440894568,\n",
       "   'video_llava': 0.38076393348900583,\n",
       "   'average': 0.39723295571751116}},\n",
       " 'cvrr_multiple_actions_in_a_single_video': {'gpt4v': {'llama_vid': 0.37500223003229083,\n",
       "   'gpt4v': 0.3798663877159728,\n",
       "   'video_chatgpt': 0.4401158903498996,\n",
       "   'mplug_owl_Video': 0.3793833490504176,\n",
       "   'video_llava': 0.379217509877277,\n",
       "   'average': 0.3907170734051716}},\n",
       " 'cvrr_time_order_understanding': {'gpt4v': {'llama_vid': 0.23355831109715908,\n",
       "   'gpt4v': 0.37167349271359185,\n",
       "   'video_chatgpt': 0.35144749290444655,\n",
       "   'mplug_owl_Video': 0.39161189034384003,\n",
       "   'video_llava': 0.35642026294850915,\n",
       "   'average': 0.34094229000150933}}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2average = {}\n",
    "judge_model.append('gpt_debate')\n",
    "for dataset, data in dataset2data.items():\n",
    "    dataset2average[dataset] = {}\n",
    "    for judge in judge_model:\n",
    "        dataset2average[dataset][judge] = {}\n",
    "        judge_total = 0\n",
    "        for candidate in candidate_model:\n",
    "            judge_total += sum(dataset2data[dataset][judge][candidate]) / len(dataset2data[dataset][judge][candidate])\n",
    "        dataset2average[dataset][judge] = judge_total / len(candidate_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvrr_continuity_and_object_instance_count': {'gpt4v': 2.201129943502825,\n",
       "  'gpt_debate': 2.074576271186441},\n",
       " 'cvrr_fine_grained_action_understanding': {'gpt4v': 2.4756521739130433,\n",
       "  'gpt_debate': 2.223478260869565},\n",
       " 'cvrr_interpretation_of_social_context': {'gpt4v': 2.322857142857143,\n",
       "  'gpt_debate': 2.137857142857143},\n",
       " 'cvrr_interpretation_of_visual_context': {'gpt4v': 2.7282051282051283,\n",
       "  'gpt_debate': 2.183882783882784},\n",
       " 'cvrr_multiple_actions_in_a_single_video': {'gpt4v': 2.3150943396226413,\n",
       "  'gpt_debate': 1.89937106918239},\n",
       " 'cvrr_time_order_understanding': {'gpt4v': 2.7197368421052635,\n",
       "  'gpt_debate': 2.025}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvrr_continuity_and_object_instance_count-gpt4v\n",
      "0.33133426981736974\n",
      "cvrr_fine_grained_action_understanding-gpt4v\n",
      "0.5039739247164825\n",
      "cvrr_interpretation_of_social_context-gpt4v\n",
      "0.5622944583205917\n",
      "cvrr_interpretation_of_visual_context-gpt4v\n",
      "0.39723295571751116\n",
      "cvrr_multiple_actions_in_a_single_video-gpt4v\n",
      "0.3907170734051716\n",
      "cvrr_time_order_understanding-gpt4v\n",
      "0.34094229000150933\n"
     ]
    }
   ],
   "source": [
    "for dataset, data in dataset2agreement.items():\n",
    "    for judge in judge_model:\n",
    "        if judge == 'gpt_debate':\n",
    "            continue\n",
    "        print(f'{dataset}-{judge}')\n",
    "        print(dataset2agreement[dataset][judge]['average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt4v': {'llama_vid': 0.23355831109715908,\n",
       "  'gpt4v': 0.37167349271359185,\n",
       "  'video_chatgpt': 0.35144749290444655,\n",
       "  'mplug_owl_Video': 0.39161189034384003,\n",
       "  'video_llava': 0.35642026294850915,\n",
       "  'average': 0.34094229000150933}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset2agreement[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
