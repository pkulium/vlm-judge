{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Data for the models\n",
    "# data = {\n",
    "#     \"Model\": [\"video_llava\", \"llama_vid\", \"gpt4o\", \"internvl2\"],\n",
    "#     \"Continuity\": [0.0085, 0.0289, 0.1173, 0.1091],\n",
    "#     \"Fine Action\": [-0.0098, -0.0047, 0.2313, 0.0983],\n",
    "#     \"Social Context\": [0.0208, -0.0104, 0.1754, 0.1124],\n",
    "#     \"Visual Context\": [0.0290, 0.0168, 0.1719, 0.1837],\n",
    "#     \"Multiple Actions\": [-0.0053, -0.0107, 0.1522, 0.0906],\n",
    "#     \"Existent Actions\": [-0.0367, -0.0547, -0.0152, 0.0115],\n",
    "#     \"Non-existent Actions\": [-0.0353, -0.0676, -0.0877, -0.0135],\n",
    "#     \"Partial Actions\": [0.0024, -0.0105, 0.0009, 0.0020],\n",
    "#     \"Time Order\": [0.0029, 0.0220, 0.0901, 0.1155],\n",
    "#     \"Emotional Context\": [-0.0016, -0.0154, 0.1184, 0.0533],\n",
    "#     \"Anomalous Activities\": [-0.0041, -0.0205, 0.0671, 0.0420],\n",
    "# }\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "# df.set_index(\"Model\", inplace=True)\n",
    "\n",
    "# # Plotting the enhanced version\n",
    "# fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# # Plotting with a more neutral color palette for clarity\n",
    "# df.T.plot(kind='bar', ax=ax, colormap='coolwarm', edgecolor='black', linewidth=1.2)\n",
    "\n",
    "# # Enhanced title and labels\n",
    "# plt.title('Agreement between MLLM judge and LLM judge on CVRR Dimensions', fontsize=18, weight='bold')\n",
    "# plt.xlabel('CVRR Dimension', fontsize=15, weight='bold')\n",
    "# plt.ylabel('Agreement', fontsize=15, weight='bold')\n",
    "\n",
    "# # Enhancing ticks\n",
    "# plt.xticks(rotation=45, fontsize=13, weight='bold')\n",
    "# plt.yticks(fontsize=13, weight='bold')\n",
    "\n",
    "# # Adding a horizontal line to indicate a performance baseline\n",
    "# plt.axhline(y=0.1, color='red', linestyle='--', linewidth=1.5, label='Baseline (y=0.1)')\n",
    "\n",
    "# # Improving legend appearance\n",
    "# plt.legend(title='Model', fontsize=12, title_fontsize=14, loc='upper left', bbox_to_anchor=(1, 1), frameon=True)\n",
    "\n",
    "# # Enhancing grid for better clarity\n",
    "# plt.grid(True, linestyle='--', linewidth=0.7, alpha=0.6)\n",
    "\n",
    "# # Adding some padding for readability\n",
    "# plt.tight_layout(pad=2)\n",
    "\n",
    "# # Save the figure with higher DPI for publication\n",
    "# file_path_publication = '/mnt/data/ICLR_Conference_Video_Language_Models_Performance_Publication_Plot.png'\n",
    "# plt.savefig(file_path_publication, dpi=300)\n",
    "\n",
    "# # Display the enhanced plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Data preparation\n",
    "# data = {\n",
    "#     \"Dimension\": [\n",
    "#         \"Continuity\", \"Fine Action\", \"Social Context\", \"Visual Context\", \n",
    "#         \"Multiple Actions\", \"Existent Actions\", \"Non-existent Actions\", \n",
    "#         \"Partial Actions\", \"Time Order\", \"Emotional Context\", \"Anomalous Activities\"\n",
    "#     ],\n",
    "#     \"Internvl2\": [3.15, 3.59, 3.39, 3.40, 3.45, 3.41, 3.13, 3.54, 3.60, 3.35, 3.27],\n",
    "#     \"Video-llava\": [3.96, 3.97, 3.97, 3.97, 3.94, 3.90, 3.92, 3.96, 3.98, 3.97, 3.98],\n",
    "#     \"Llama-vid\": [3.82, 3.85, 3.89, 3.89, 3.75, 3.86, 3.77, 3.82, 3.84, 3.84, 3.84],\n",
    "#     \"GPT4o\": [2.46, 3.06, 2.66, 2.73, 2.56, 2.56, 2.37, 2.82, 3.14, 2.80, 2.45],\n",
    "#     \"GPT3.5\": [2.62, 2.68, 2.60, 2.59, 2.35, 1.75, 1.77, 2.29, 2.45, 2.21, 2.26]\n",
    "# }\n",
    "\n",
    "# # Create DataFrame\n",
    "# df_short = pd.DataFrame(data)\n",
    "# df_short.set_index('Dimension', inplace=True)\n",
    "\n",
    "# # Plotting the heatmap\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(df_short, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "# plt.title('Heatmap of Judge Model Scores by Dimension')\n",
    "# plt.xticks(rotation=45, ha='right')  # Rotate model names for better readability\n",
    "# plt.yticks(rotation=0)\n",
    "# plt.tight_layout()  # Adjust layout to not cut off labels\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "# Load the JSON data from the file\n",
    "file_path = '../logs_phase1_processed/batch.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Collect the length of responses for each model across all datasets\n",
    "length_distribution = defaultdict(list)\n",
    "\n",
    "for dataset_name, dataset_records in data.items():\n",
    "    for record_id, responses in dataset_records.items():\n",
    "        for model, response in responses.items():\n",
    "            if model != 'target':  # Exclude 'target'\n",
    "                length_distribution[model].append(len(response[0]))\n",
    "\n",
    "# Display the length distribution\n",
    "for model, lengths in length_distribution.items():\n",
    "    # lengths = lengths[:10]\n",
    "    print(f\"Model: {model}, Lengths: {lengths}\")\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "\n",
    "# # Plotting the KDE (Kernel Density Estimate) distribution for each model\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# for model, lengths in length_distribution.items():\n",
    "#     sns.kdeplot(lengths, label=model, shade=True, bw_adjust=1.2)\n",
    "\n",
    "# plt.title('Response Collection Length Distribution')\n",
    "# plt.xlabel('Response Length')\n",
    "# plt.ylabel('Density')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.grid(False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: llama_vid, Lengths: 4663\n",
      "Model: gpt4v, Lengths: 2091\n",
      "Model: video_chatgpt, Lengths: 4631\n",
      "Model: mplug_owl_Video, Lengths: 627\n",
      "Model: video_llava, Lengths: 3975\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Prepare the data\n",
    "data = {\n",
    "    'cvrr_continuity_and_object_instance_count': {'llama_vid': {'video_llava': 3.9548022598870056, 'llama_vid': 3.8079096045197742, 'gpt4v': 2.5141242937853105, 'internvl2': 3.1186440677966103, 'gpt3.5': 2.305084745762712},\n",
    "    'gpt4v': {'video_llava': 3.8587570621468927, 'llama_vid': 3.7062146892655368, 'gpt4v': 2.8135593220338984, 'internvl2': 3.135593220338983, 'gpt3.5': 3.4124293785310735},\n",
    "    'video_chatgpt': {'video_llava': 3.9887005649717513, 'llama_vid': 3.830508474576271, 'gpt4v': 2.169491525423729, 'internvl2': 3.016949152542373, 'gpt3.5': 2.6553672316384183},\n",
    "    'mplug_owl_Video': {'video_llava': 4.0, 'llama_vid': 3.903954802259887, 'gpt4v': 2.440677966101695, 'internvl2': 3.2203389830508473, 'gpt3.5': 2.2655367231638417},\n",
    "    'video_llava': {'video_llava': 3.983050847457627, 'llama_vid': 3.8700564971751414, 'gpt4v': 2.384180790960452, 'internvl2': 3.2824858757062145, 'gpt3.5': 2.463276836158192}}\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({(outerKey, innerKey): values for outerKey, innerDict in data.items() for innerKey, values in innerDict.items()})\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", cbar_kws={'label': 'Performance Score'})\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Model Performance Across Different CVRR Datasets\", fontsize=16)\n",
    "plt.xlabel(\"Judge Models\", fontsize=12)\n",
    "plt.ylabel(\"Candidate Models\", fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
