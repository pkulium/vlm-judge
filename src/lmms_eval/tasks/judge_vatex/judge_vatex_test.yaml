dataset_path: lmms-lab/VATEX
dataset_name: vatex_test
task: judge_vatex_test
test_split: test
output_type: generate_until
doc_to_visual: !function utils.vatex_test_doc_to_visual
doc_to_text: !function utils.vatex_test_doc_to_text
doc_to_target: "answer"

process_results: !function utils.vatex_process_result
# Note that the metric name can be either a registed metric function (such as the case for GQA) or a key name returned by process_results
metric_list:
  - metric: vatex_Bleu_4
    aggregation: !function utils.vatex_bleu4
    higher_is_better: true
  - metric: vatex_METEOR
    aggregation: !function utils.vatex_meteor
    higher_is_better: true
  - metric: vatex_ROUGE_L
    aggregation: !function utils.vatex_rougel
    higher_is_better: true
  - metric: vatex_CIDEr
    aggregation: !function utils.vatex_cider
    higher_is_better: true
metadata:
  version: 0.0

dataset_kwargs:
  token: True
  video: True #skip download video from hf
  # force_unzip: True
  cache_dir: vatex_test
  # From_YouTube: True

model_specific_prompt_kwargs:
  default:
    prompt: Provide a rating(on a scale 1 to 5) for answer from the assistant. Give your reasoning. You must start your response with 'rating number is'.
  gemini_api:
    prompt: Provide a rating(on a scale 1 to 5) for answer from the assistant. Give your reasoning. You must start your response with 'rating number is'.

generation_kwargs:
  max_new_tokens: 256
  temperature: 0
  top_p: 1.0
  num_beams: 1
  do_sample: false
